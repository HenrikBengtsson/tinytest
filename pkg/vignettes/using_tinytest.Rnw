%\VignetteIndexEntry{Using tinytest}
\documentclass[11pt]{article}
\usepackage{enumitem}
\setlist{nosep}

\usepackage{hyperref}

\hypersetup{
  pdfborder={0 0 0}
 , colorlinks=true 
 , urlcolor=red
 , linkcolor=blue
 , citecolor=blue
}

\renewcommand{\familydefault}{\sfdefault}


\title{Using tinytest}
\author{Mark van der Loo}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}


\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}



\begin{document}
\newlength{\fancyvrbtopsep}
\newlength{\fancyvrbpartopsep}
\makeatletter
\FV@AddToHook{\FV@ListParameterHook}{\topsep=\fancyvrbtopsep\partopsep=\fancyvrbpartopsep}
\makeatother


\setlength{\fancyvrbtopsep}{0pt}
\setlength{\fancyvrbpartopsep}{0pt}
\maketitle{}
\hfill\emph{The purpose of testing is to gain confidence in your code.}
\tableofcontents{}
<<echo=FALSE>>=
options(prompt="    ", continue = "    ")
@


\subsection*{Before you read this}
I expect that readers of this document know how to write R functions. If you
want to use \pkg{tinytest} for your package, I expect that you have a basic
understanding of the directory structure that constitutes the source of an R
package.

\newpage{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Purpose of this package: unit testing}

The purpose of \emph{unit testing} is to check whether a function gives the
output you expect, when it is provided with certain input. So unit testing is
all about comparing \emph{realized} outputs with \emph{desired} outputs. The
purpose of this package is to facilitate writing unit tests, and to support a
workflow of writing tests while improving code.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Expressing tests}
Suppose we define a function translating pounds (lbs) to kilograms.
<<>>=
lbs2kg <- function(x){
  if ( x < 0 ){
    stop(sprintf("Expected nonnegative weight, got %g",x))
  }
  x/2.20
}
@
We like to check a few things before we trust it.
<<>>=
library(tinytest)
expect_equal(1/2.2046, lbs2kg(1))
expect_error(lbs2kg(-3))
@

The value of an \code{expect\_*} function is a \code{logical}, with some
attributes that record differences, if there are any. These attributes are used
to pretty-print the results.
<<>>=
isTRUE( expect_true(2 == 1 + 1) )
@



\subsection{Test functions}
Currently, the following expectations are implemented.
\begin{center}
\begin{tabular}{ll}
\textbf{Function} & \textbf{what it looks for}\\
\code{expect\_equal(target, current)}      & exact equality\\
\code{expect\_equivalent(target, current)} & equality, ignoring attributes\\
\code{expect\_true(current)}               & does `current' evaluate to \code{TRUE}\\
\code{expect\_false(current)}              & does `current' evaluate to \code{FALSE}\\
\code{expect\_error(current, pattern)}     & error message matching \code{pattern}\\
\code{expect\_warning(current, pattern)}   & warning message matching \code{pattern}
\end{tabular}
\end{center}
Here, \code{target} is the intended outcome and current is the resulting
outcome.  Also, \code{pattern} is interpreted as a regular expression. 
<<>>=
expect_error(lbs2kg(-3), pattern="nonnegative")
expect_error(lbs2kg(-3), pattern="foo")
@


\subsection{Interpreting the output}
Let's have a look at an example again.
<<>>=
expect_false( 1 + 1 == 2 )
@

The output of these functions is pretty self-explanatory, nevertheless we see that
the output of these expect-functions consist of
\begin{itemize}
\item The result: \code{FAILED} or \code{PASSED}.
  \item The type of failure (if any) between square brackets. Current options
       are as follows.
  \begin{itemize}
    \item \code{[data]} there are differences between observed and expected values.
    \item \code{[attr]} there are differences between observed and expected attributes, such as column names.
    \item \code{[xcpt]} an exception (warning, error) was expected but not observed. 
  \end{itemize}
  \item When relevant (see \S\ref{sect:testfiles}), the location of the test file and the relevant line numbers.
  \item When necessary, a summary of the differences between observed and expected
    values or attributes.
  \item The test call.
\end{itemize}
It is possible to print more prudent output, with a oneliner for each test.
<<>>=
print(expect_equal(1+1, 3), type="short")
@
Tinytest will switch between long and short formats, depending on the workflow it
is used in. But you can always set 
<<eval=FALSE>>=
options(tt.print="long")
@
to force long (or \code{"short"} output. The default value is \code{"auto"}.




\section{Test files}
\label{sect:testfiles}
In \pkg{tinytest}, tests are scripts, interspersed with statements that perform
checks. An example test file in tinytest can look like this.


\begin{verbatim}
    # contents of test_women.R
    library(tinytest)
    m <- mean(women$height)
    expect_equal(sum(women$height)/nrow(women), m)
    
\end{verbatim}

A particular file can be run using
\code{run\_test\_file("path/to/test\_file.R")}. Only failing tests will be
printed. If you want to run all tests files in a directory, do
\code{run\_test\_dir("path/to/test/dir")}. By default, all \code{.R} or
\code{.r} files starting with \code{test} will be run.


\textbf{NOTE.} \code{run\_test\_dir("dir")} will temporarily set the working 
directory to \code{"dir"}, which comes in handy when comparing output to data
stored in files. (See \S\ref{sect:comparefiles}). \code{run\_test\_file} does
not do that. If you want to test a single file say \code{dir/test\_foo.R},
while making sure that the working directory is \code{dir} during execution,
you can do
<<eval=FALSE>>=
run_test_dir("path/to/dir", pattern="test_foo")
@

\section{Testing packages}

Using \pkg{tinitest} for your package is pretty easy.
\begin{enumerate}
\item Testfiles are placed in \code{/inst/utst}. The testfiles all have
names starting with \code{test} (for example \code{test\_haha.R}).
\item In the file \code{/tests/tinytest.R} you place the code
\begin{verbatim}
    if ( require(tinytest, quietly=TRUE) ){
      test_package("PACKAGENAME")
    }
\end{verbatim}
\item In your \code{DESCRIPTION} file, add \pkg{tinytest} to \code{Suggests:}.
\end{enumerate}

In a terminal, you can now do
\begin{verbatim}
    R CMD build /path/to/your/package
    R CMD check PACKAGENAME_X.Y.Z.tar.gz
\end{verbatim}
and all tests will run.

To run all the tests interactively, make sure that all functions of your 
new package are loaded. After that, run
<<eval=FALSE>>=
test_all("/path/to/your/package")
@
where the default package directory is the current working directory.




\subsection{Using data stored in files}
\label{sect:comparefiles}
When your package is tested with \code{test\_package}, \pkg{tinytest} ensures
that your working directory is the testing directory (by default \code{utst}).
This means you can files that are stored in your folder directly.

Suppose that your package directory structure looks like this (default):
\begin{itemize}
\item[]\code{/inst}
  \begin{itemize}
    \item[]\code{/utst}
    \begin{itemize}
       \item[]\code{/test.R}
       \item[]\code{/women.csv}
    \end{itemize}
  \end{itemize}
\end{itemize}

Then, to check whether the contents of \code{women.csv} is equal to the 
built-in \code{women} dataset,  the content of \code{test.R} looks as follows.
<<eval=FALSE>>=
dat <- read.csv("women.csv")
expect_equal(women, dat)
@

\textbf{Note.} This will work with \code{test\_all()} and with \code{R CMD
check} but not with \code{run\_test\_file()} because the latter does not
temporarily change working directory.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Skipping tests on \code{CRAN}}
There are limits to the amount of time a test can take on CRAN. For longer
running code it is desirable to automatically toggle these tests off on CRAN,
but to run them during development. 

You can not really skip tests at CRAN, because there is no certain way to
detect whether a package is tested at one of the CRAN's machines. However,
tests that are run with \code{test\_all} or \code{test\_dir} can be toggled on
and off as follows.
<<>>=
if ( at_home() ){
  expect_equal(2, 1+1)
}
@
If a test file is run using \code{test\_all} or \code{test\_dir} then by
default the code following the if-conditions is executed. It will be skipped on
CRAN since tests are initiated with \code{test\_package} in that case.
It is possible to switch the tests off by \code{test\_all(..., at\_home=FALSE)}
and similar for \code{test\_dir}.





\section{A few tips on packages and unit testing}


\subsection{Make your package spherical}

Larger packages typically consist of a functions that are visible to the
users of that package (exported functions) as well as a bunch of functions
that are used by the exported functions. For example:

<<>>=
# exported, user-visible function
inch2cm <- function(x){
	x*conversion_factor("inch")
}
# not exported function, package-internal
conversion_factor <- function(unit){
	confac <- c(inch=2.54, pound=1/2.2056)
  confac[unit]
}
@

We can think of the exported functions as the \emph{surface} of your package,
and all the other functions as the \emph{volume}. The surface is what a user
sees, the volume is what the developer sees. The surface is how a user
interacts with a package. 


If the surface is small (few functions exported), users are limited in the ways
they can interact with your package, and that means there is less to test. So
as a rule of thumb, it is a good idea to keep the surface small. Since a sphere
has the smallest surface-to-volume ratio possible, I refer to this rule as
\emph{keep your package spherical}.

By the way, the technical term for the surface of a package is API (application
program interface).




\subsection{Test the surface, not the volume}

Unexpected behavior (a bug) is often discovered when someone who is not the
developer starts using code. Bugfixing implies altering code and it may even
require you to refactor large chunks of code that is internal to a package.  If
you defined extensive tests on non-exported functions, this means you need to
rewrite the tests as well. As a rule of thumb, it is a good idea to test only
the behaviour at the surface, so as a developer you have more freedom to change
the internals. This includes rewriting and renaming internal functions completely.


By the way, it is bad practice to change the surface, since that means you are
going to break other people's code. Nobody likes to program against an API that
changes frequently, and everybody hates to program against an API that changes
unexpectedly.

\subsection{How many tests do I need?}
In Section 1 we talked about pure functions, and how they can in principle be
replaced with a table connecting input to output. To test a function you do
not need to compare each possible input to each possible output. 

When you call a function, you can think of it's arguments flowing through a certain
path from input to output. As an example, let's take a look again at a new, slightly
safer unit conversion function.
<<>>=
pound2kg <- function(x){
  stopifnot( is.numeric(x) )
  if ( any(x < 0) ){
    warning("Found negative input, converting to positive")
    x <- abs(x)
  }
  x/2.2046
}
@
If we call \code{lbs2kg} with argument \code{2}, we can write:
\begin{verbatim}
    2 -> /2.2046 -> output
\end{verbatim}
If we call \code{lbs2kg} with argument \code{-3} we can write
\begin{verbatim}
   -3 -> abs() -> /2.2046 -> output
\end{verbatim}
Finally, if we call \code{pound2kg} with \code{"foo"} we can write
\begin{verbatim}
   "foo" -> stop() -> Exception
\end{verbatim}

So we have three possible paths. In fact, we see that every nonnegative number
will follow the first path, every negative number will follow the second path
and anything nonnumeric follows the third path. So the following test suite
fully tests the behaviour of our function.
<<eval=FALSE>>=
    expect_equal( 1/2.2046, pound2kg(1) )
    # test for expected warning, store output
    expect_warning( out <- pound2kg(-1) )
    # test the output
    expect_equal( 1/2.2046, out )
    expect_error(pound2kg("foo"))
@

The number of paths of a function is called its \emph{cyclomatic complexity}.
For larger functions, with multiple arguments, the number of paths typically
grows extremely fast, and it quickly becomes impossible to define a test for
each and every one of them. If you want to get an impression of how many tests
one of your functions in needs in principle, you can have a look at the
\pkg{cyclocomp} package of G\'abor Cs\'ardi\cite{csardi2016cyclocomp}.

Since full path coverage is out of range in most cases, developers often strive
for something simple, which is full code coverage, which simply means that each
line of code is run in at least one test. Full code coverage is no guarantee
for bugfree code. Besides code coverage it is therefore a good idea to think
about the various ways a user might use your code and include tests for that.

To measure code coverage, I recommend using the \pkg{covr} package by Jim
Hester\cite{hester2018covr}. Since \pkg{covr} is independent of the tools or
packages used for testing, it also works fine with \pkg{tinytest}.


\subsection{It's not a bug, it's a test!}
If users of your code are friendly enough to submit a bug report when they find
one, it is a good idea to start by writing a small test that reproduces the
error and add that to your test suite. That way, whenever you work on your
code, you can be sure to be alarmed when a bug reappears.

Tests that represent earlier bugs are sometimes called \emph{regression tests}.
If a bug reappears during development, software engineers sometimes refer
to this as a \emph{regression}.




\begin{thebibliography}{5}
  \bibitem{csardi2016cyclocomp}
    \href{https://CRAN.R-project.org/package=cyclocomp}{cyclocomp: Cyclomatic Complexity of R Code}
    G\'abor Cs\'ardi (2016)
    R package version 1.1.0

  \bibitem{hester2018covr}
    \href{https://CRAN.R-project.org/package=covr}{covr: Test Coverage for Packages}
    Jim Hester (2018)
    R package version 3.2.1

  \bibitem{eddelbuettel2016SO}
   \href{https://stackoverflow.com/questions/36166288/skip-tests-on-cran-but-run-locally}{Skip tests on CRAN, but run locally}
   Answered by Dirk Eddelbuettel (2016)
  
  \bibitem{wickham2011testthat}
   \href{https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf}{testthat: Get Started with Testing}
   H. Wickham (2011). R Journal \textbf{3} 5--10.

  \bibitem{wickham2018devtools}
   \href{https://CRAN.R-project.org/package=devtools}{devtools: Tools to Make Developing R Packages Easier}
   H. Wickham, J. Hester, W Chang (2018) R package version 2.0.1

\end{thebibliography}

\end{document}
